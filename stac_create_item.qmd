---
title: "STAC create item "
format: html
jupyter: titiler  # Use your Conda environment
---

```{r env-activate}
library(reticulate)
reticulate::use_condaenv("stac", required = TRUE)
# reticulate::py_config()

```


```{python date-extract}
import re

def date_extract_after_utm(s):
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    return match.group(1) if match else None

# this is more flexible 
def date_extract_from_path(s):
    # Step 1: Try to extract YYYYMMDD or YYYY after _utmXX_
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    if match:
        val = match.group(1)
        if val.isdigit():
            year = int(val[:4])
            if 2000 <= year <= 2050:
                return val

    # Step 2: Fallback: look for /YYYY/ in the path
    fallback = re.search(r'/([2][0-9]{3})/', s)
    if fallback:
        year = int(fallback.group(1))
        if 2000 <= year <= 2050:
            return str(year)

    return None


from datetime import datetime, timezone

def datetime_parse_item(s):
    if s is None:
        return None
    if len(s) == 8:
        return datetime.strptime(s, "%Y%m%d").replace(tzinfo=timezone.utc)
    elif len(s) == 4:
        return datetime.strptime(s, "%Y").replace(tzinfo=timezone.utc)
    return None


```


```{python create-stac-items}
import concurrent.futures
import pathlib
import json
from datetime import datetime
import pystac
import rasterio
import rio_stac.stac
import subprocess
import pandas as pd
import os
import argparse
from pystac import Link, RelType


# Parse CLI arguments
test_only = True
test_number_items = 10

# Define paths
path_local = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"

# TEST!!!!!!!!!!!!!!!!!!!!
# path_local = "/Users/airvine/Projects/gis/stac/test"


path_collection = f"{path_local}/collection.json"
path_s3_stac = "https://stac-orthophoto-bc.s3.amazonaws.com"
path_s3_json = f"{path_s3_stac}/collection.json"
path_s3 = "https://nrs.objectstore.gov.bc.ca/gdwuts"
path_results_csv = "data/stac_geotiff_checks.csv"

# Load base collection
collection = pystac.Collection.from_file(path_collection)
collection.set_self_href(path_s3_json)

# Load list of image paths
with open("data/urls_list.txt") as f:
    path_items = f.read().splitlines()

# Determine URLs to check
if test_only:
    urls_to_check = path_items[:test_number_items]
else:   
    urls_to_check = path_items


# Validate GeoTIFF and COG in one step
def check_geotiff_cog(url: str) -> dict:
    try:
        result = subprocess.run(
            ["rio", "cogeo", "validate", f"/vsicurl/{url}"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False
        )
        output = result.stdout.decode() + result.stderr.decode()
        return {
            "url": url,
            "is_geotiff": "is NOT a valid cloud optimized GeoTIFF" in output or "is a valid cloud optimized GeoTIFF" in output,
            "is_cog": "is a valid cloud optimized GeoTIFF" in output
        }
    except FileNotFoundError:
        raise RuntimeError("`rio cogeo` is not installed or not in PATH")

print(f"Checking {len(urls_to_check)} URLs{' (forced)' if check_all else ' (new only)'}...")

# Run checks
import tqdm
with concurrent.futures.ThreadPoolExecutor() as executor:
    new_results = list(tqdm.tqdm(executor.map(check_geotiff_cog, urls_to_check), total=len(urls_to_check), desc="Validating GeoTIFFs"))

# Combine and save
df_new = pd.DataFrame(new_results)
df_all = df_new if check_all else pd.concat([df_existing, df_new], ignore_index=True)
df_all.to_csv(path_results_csv, index=False)

# Create lookup for quick access
results_lookup = {
    row["url"].replace("https:/", "https://"): {"is_geotiff": row["is_geotiff"], "is_cog": row["is_cog"]}
    for _, row in df_all.iterrows()
}

# Define the processing function
def process_item(path_item: str) -> dict | None:
    href_item = path_item.replace("https:/", "https://")
    check = results_lookup.get(href_item)
    if check is None or not check["is_geotiff"]:
        print(f"Skipping unreadable GeoTIFF: {href_item}")
        return None

    item_id = path_item[len(path_s3):].replace("/", "-").removesuffix(".tif")
    item_time = datetime_parse_item(date_extract_from_path(path_item))

    media_type = (
        "image/tiff; application=geotiff; profile=cloud-optimized"
        if check["is_cog"] else
        "image/tiff; application=geotiff"
    )

    try:
        item = rio_stac.stac.create_stac_item(
            path_item,
            id=item_id,
            asset_media_type=media_type,
            asset_name='image',
            asset_href=href_item,
            with_proj=True,
            collection=collection.id,
            collection_url=path_s3_json,
            asset_roles=["data"]
        )
        item.datetime = item_time
        item.assets['image'].href = href_item

        # Save locally
        path_item_json = f"{path_local}/{item_id}.json"
        item.save_object(dest_href=path_item_json, include_self_link=False)

        return {
            "id": item_id,
            "item": item
            # "media_type": media_type
        }
    except Exception as e:
        print(f"Error processing {href_item}: {e}")
        return None

# Run in parallel using threads to avoid rasterio multiprocessing issues
try:
    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = list(filter(None, tqdm.tqdm(executor.map(process_item, urls_to_check), total=len(urls_to_check), desc="Creating STAC Items")))
except Exception as e:
    print(f"Parallel execution failed: {e}")
    results = []

# Add all valid item json href to the collection
if results:
    for result in results:
        collection.add_link(Link(
            rel=RelType.ITEM,
            target=f"{path_s3_stac}/{result['id']}.json",
            media_type="application/json"
        ))

collection.save_object(dest_href=path_collection)



```


```{r json-clean, eval= FALSE}
# DELETE ATTACK - USE WITH CARE
# list all the jsons in the directory so we can delete them
path_local = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"
# TEST!!!
# path_local = "/Users/airvine/Projects/gis/stac/test"
j <- fs::dir_ls(path_local, glob = "*.json", recurse = TRUE)

fs::file_delete(j)

```

```{python validate}
for item in collection.get_all_items():
    try:
        item.validate()
        # print(f"✅ {item.id} is valid.")
    except Exception as e:
        print(f"❌ {item.id} failed validation: {e}")

# or 1 at a time
# item = Item.from_file(f"{path_local}/093-093h-2019-dem-bc_093h092_xli1m_utm10_2019.json")
# item.validate()
```

```{r hack-collection-repair, eval=FALSE}
# we had an error we are reparing here manually rather than rerun for 5 hours
# Load and modify the file
path <- "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc/collection.json"

# Read the file
lines <- readLines(path)

# Replace all instances of the MIME type
lines <- gsub(
  pattern = '"type": "image/tiff; application=geotiff"',
  replacement = '"type": "application/json"',
  x = lines,
  fixed = TRUE
)

# Write back to the same file
writeLines(lines, path)
```

