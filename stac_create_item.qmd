---
title: "STAC create item "
format: html
jupyter: titiler  # Use your Conda environment
---

```{r env-activate}
library(reticulate)
reticulate::use_condaenv("stac", required = TRUE)
# reticulate::py_config()

```


```{python import-modules}
import pathlib
import pystac
import rio_stac
```

```{python date-extract}
import re

def date_extract_after_utm(s):
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    return match.group(1) if match else None

# this is more flexible 
def date_extract_from_path(s):
    # Step 1: Try to extract YYYYMMDD or YYYY after _utmXX_
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    if match:
        val = match.group(1)
        if val.isdigit():
            year = int(val[:4])
            if 2000 <= year <= 2050:
                return val

    # Step 2: Fallback: look for /YYYY/ in the path
    fallback = re.search(r'/([2][0-9]{3})/', s)
    if fallback:
        year = int(fallback.group(1))
        if 2000 <= year <= 2050:
            return str(year)

    return None


from datetime import datetime, timezone

def datetime_parse_item(s):
    if s is None:
        return None
    if len(s) == 8:
        return datetime.strptime(s, "%Y%m%d").replace(tzinfo=timezone.utc)
    elif len(s) == 4:
        return datetime.strptime(s, "%Y").replace(tzinfo=timezone.utc)
    return None


```


```{python stac-item-create}
# Define the path to the STAC collection JSON file.  

path_local = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"
path_collection = f"{path_local}/collection.json"

# Define the path to the STAC collection JSON file on S3.  Seems a bit weird that we actually
# need to put a file here in able to build our collection
path_s3_stac = "https://stac-orthophoto-bc.s3.amazonaws.com"
path_s3_json = f"{path_s3_stac}/collection.json"

# Define the base local path where imagery is stored
# path_base = pathlib.Path("/Users/airvine/Projects/gis/stac_orthophoto_bc/stac/dev/stac_orthophoto_bc")

# Define the base S3 URL where assets are hosted
path_s3 = "https://nrs.objectstore.gov.bc.ca/gdwuts"

# Load the existing STAC collection from the file
collection = pystac.Collection.from_file(path_collection)

# Manually set the self HREF for the collection to the cloud location
collection.set_self_href(path_s3_json)

# Define a list of paths to process
# path_items = list(path_base.rglob("*.tif"))

with open("data/urls_list.txt") as f:
    path_items = f.read().splitlines()

# Iterate over each image file in the list
for path_item in path_items:
    # Compute the relative path of the item within the base directory
    href_item = path_item.replace("https:/", "https://")
    item_id = path_item[len(path_s3):].replace("/", "-").removesuffix(".tif")
    item_time = datetime_parse_item(date_extract_from_path(path_item))


    # Create a STAC Item for the given image file
    item = rio_stac.stac.create_stac_item(
        path_item, 
        id=item_id,
        asset_media_type='image/tiff; application=geotiff; profile=cloud-optimized',
        asset_name='image',
        asset_href=href_item,
        with_proj=True,  # Extract and include projection properties
        collection=collection.id,  # Assign the item to the collection
        collection_url=collection.get_self_href(),  # Use collection's self href
        asset_roles=["data"]
    )
    
    # becasue we use rio we need to add the datetime after
    item.datetime = item_time

    # Add the newly created item to the collection
    collection.add_item(item)
    
    # Define the correct self_href for the item JSON file
    item_self_href = f"{path_s3_stac}/{item_id}.json"
    item.set_self_href(item_self_href)

    # Print the asset HREF for debugging purposes
    print(f"Asset HREF after editing: {item.assets['image'].href}")

    # Save the STAC item JSON in the main directory 
    path_item_json = f"{path_local}/{item_id}.json"
    item.save_object(dest_href=path_item_json)


# Save the updated STAC collection with the new items included
collection.save_object(
    # include_self_link=True, 
    dest_href=path_collection
    )

```

```{python, parallel-stac-item-create}
import concurrent.futures
import pathlib
import json
from datetime import datetime
import pystac
import rasterio
import rio_stac.stac
import subprocess
import pandas as pd
import os
import argparse

# Parse CLI arguments
# Set check_all to True to revalidate all URLs
check_all = True

# Define paths
path_local = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"
path_collection = f"{path_local}/collection.json"
path_s3_stac = "https://stac-orthophoto-bc.s3.amazonaws.com"
path_s3_json = f"{path_s3_stac}/collection.json"
path_s3 = "https://nrs.objectstore.gov.bc.ca/gdwuts"
path_results_csv = "data/data/stac_geotiff_checks.csv"

# Load base collection
collection = pystac.Collection.from_file(path_collection)
collection.set_self_href(path_s3_json)

# Load list of image paths
with open("data/urls_list.txt") as f:
    path_items = f.read().splitlines()
    
# jsut for testing!!!!
path_items = path_items[:20]

# Load previous validation results
if os.path.exists(path_results_csv) and not check_all:
    df_existing = pd.read_csv(path_results_csv)
    done_urls = set(df_existing["url"])
else:
    df_existing = pd.DataFrame(columns=["url", "is_geotiff", "is_cog"])
    done_urls = set()

# Validate GeoTIFF and COG in one step
def check_geotiff_cog(url: str) -> dict:
    try:
        result = subprocess.run(
            ["rio", "cogeo", "validate", f"/vsicurl/{url}"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False
        )
        output = result.stdout.decode()
        return {
            "url": url,
            "is_geotiff": "The following errors were found:" in output or "is a valid cloud optimized GeoTIFF" in output,
            "is_cog": "is a valid cloud optimized GeoTIFF" in output
        }
    except FileNotFoundError:
        raise RuntimeError("`rio cogeo` is not installed or not in PATH")

# Run validation only on new URLs unless --force is set
urls_to_check = path_items if args.force else [u for u in path_items if u not in done_urls]

print(f"Checking {len(urls_to_check)} URLs{' (forced)' if args.force else ' (new only)'}...")

# Run checks
with concurrent.futures.ThreadPoolExecutor() as executor:
    new_results = list(executor.map(check_geotiff_cog, urls_to_check))

# Combine and save
df_new = pd.DataFrame(new_results)
df_all = df_new if args.force else pd.concat([df_existing, df_new], ignore_index=True)
df_all.to_csv(path_results_csv, index=False)

# Create lookup for quick access
results_lookup = {
    row["url"]: {"is_geotiff": row["is_geotiff"], "is_cog": row["is_cog"]}
    for _, row in df_all.iterrows()
}

# Define the processing function
def process_item(path_item: str) -> dict | None:
    href_item = path_item.replace("https:/", "https://")
    check = results_lookup.get(href_item)
    if check is None or not check["is_geotiff"]:
        print(f"Skipping unreadable GeoTIFF: {href_item}")
        return None

    item_id = path_item[len(path_s3):].replace("/", "-").removesuffix(".tif")
    item_time = datetime_parse_item(date_extract_from_path(path_item))

    media_type = (
        "image/tiff; application=geotiff; profile=cloud-optimized"
        if check["is_cog"] else
        "image/tiff; application=geotiff"
    )

    try:
        item = rio_stac.stac.create_stac_item(
            path_item,
            id=item_id,
            asset_media_type=media_type,
            asset_name='image',
            asset_href=href_item,
            with_proj=True,
            collection=collection.id,
            collection_url=path_s3_json,
            asset_roles=["data"]
        )
        item.datetime = item_time
        item.set_self_href(f"{path_s3_stac}/{item_id}.json")

        # Save locally
        path_item_json = f"{path_local}/{item_id}.json"
        item.save_object(dest_href=path_item_json)

        return {
            "id": item_id,
            "json_path": path_item_json
        }
    except Exception as e:
        print(f"Error processing {href_item}: {e}")
        return None

# Run in parallel using threads to avoid rasterio multiprocessing issues
try:
    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = list(filter(None, executor.map(process_item, path_items)))
except Exception as e:
    print(f"Parallel execution failed: {e}")
    results = []

# Add all valid items to the collection
if results:
    for result in results:
        item = pystac.Item.from_file(result["json_path"])
        collection.add_item(item)

# Save final collection
collection.save_object(dest_href=path_collection)

```


```{r json-clean, eval= FALSE}
# DELETE ATTACK - USE WITH CARE
# list all the jsons in the directory so we can delete them
path_base = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"

j <- fs::dir_ls(path_base, glob = "*.json", recurse = TRUE)

fs::file_delete(j)

```

```{python validate}


for item in collection.get_all_items():
    try:
        item.validate()
        # print(f"✅ {item.id} is valid.")
    except Exception as e:
        print(f"❌ {item.id} failed validation: {e}")


# or 1 at a time
item = Item.from_file(f"{path_local}/093-093h-2019-dem-bc_093h092_xli1m_utm10_2019.json")
item.validate()
```


```{python, xyz, eval - FALSE}
import mercantile

# Bounds from Titiler response
west, south, east, north = -124.703866, 54.107650, -124.695957, 54.111509

# Choose a zoom level
zoom = 15

# Get a tile covering the center of the bounds
tile = mercantile.tile((west + east) / 2, (south + north) / 2, zoom)

print(f"Tile coordinates: Z={tile.z}, X={tile.x}, Y={tile.y}")
```

```{python query-s3-json}
import pystac

# Load the Collection JSON directly
collection = pystac.Collection.from_file("https://dev-imagery-uav-bc.s3.amazonaws.com/imagery_uav_bc.json")

# Define BC bounding box (approximate)
bbox_bc = [-139, 48, -114, 60]  # (West, South, East, North)

# Get all items linked in the collection
items = collection.get_all_items()

# Filter items that fall within BC bounds
bc_items = []
for item in items:
    # Check if item's bounding box intersects with BC
    item_bbox = item.bbox  # [west, south, east, north]
    if (
        item_bbox[2] >= bbox_bc[0] and  # Item east >= BC west
        item_bbox[0] <= bbox_bc[2] and  # Item west <= BC east
        item_bbox[3] >= bbox_bc[1] and  # Item north >= BC south
        item_bbox[1] <= bbox_bc[3]      # Item south <= BC north
    ):
        bc_items.append(item)

# Print matching items
for item in bc_items:
    print(f"Item: {item.id}")
    for asset_key, asset in item.assets.items():
        print(f"  - {asset_key}: {asset.href}")


```


```{python query-titiler}
import requests
import pystac

# Define BC bounding box (West, South, East, North)
bbox_bc = [-139, 48, -114, 60]

# Titiler endpoint
titiler_endpoint = "http://titiler-env.eba-s4jhubvr.us-west-2.elasticbeanstalk.com"

# Load the STAC Collection
collection_url = "https://dev-imagery-uav-bc.s3.amazonaws.com/imagery_uav_bc.json"
collection = pystac.Collection.from_file(collection_url)

# Get all items
items = collection.get_all_items()

# Filter items using /stac/bounds
matching_items = []
for item in items:
    stac_url = item.get_self_href()
    bounds_url = f"{titiler_endpoint}/stac/bounds?url={stac_url}"

    response = requests.get(bounds_url)
    if response.status_code == 200:
        item_bbox = response.json()["bounds"]  # [west, south, east, north]

        # Check intersection with BC bbox
        if (
            item_bbox[2] >= bbox_bc[0] and  # Item east >= BC west
            item_bbox[0] <= bbox_bc[2] and  # Item west <= BC east
            item_bbox[3] >= bbox_bc[1] and  # Item north >= BC south
            item_bbox[1] <= bbox_bc[3]      # Item south <= BC north
        ):
            matching_items.append((item.id, stac_url))

# Print matching items
print("Matching Items in BC:")
for item_id, stac_url in matching_items:
    print(f"- {item_id}: {stac_url}")


```

```{python footprint}

# here we explore how to update the footprints of our items so that we see where the raster has non-na values
# Remote URL wrapped in /vsicurl/ for GDAL
url = "/vsicurl/https://nrs.objectstore.gov.bc.ca/gdwuts/093/093h/2019/dem/bc_093h092_xli1m_utm10_2019.tif"
url = "/vsicurl/https://nrs.objectstore.gov.bc.ca/gdwuts/093/093h/2019/dem/bc_093h084_xli1m_utm10_2019.tif"

from stactools.core.utils.raster_footprint import RasterFootprint
from shapely.geometry import shape, mapping, Polygon

# Build footprint
rf = RasterFootprint.from_href(
    href=url,
    precision=5,                # Coarser geometry (faster, smaller)
    densification_factor=None, # Disable extra vertex interpolation
    densification_distance=None,
    simplify_tolerance=0.001,  # Simplify geometry slightly (in degrees) 0.001 ≈ 111 meters
    no_data=None,              # Use nodata from metadata (excludes it automatically)
    bands=[1]                  # Only use first band
)
geojson_dict = rf.footprint()

# Optional: turn into shapely geometry
geom = shape(geojson_dict)

# Convert back to GeoJSON dict
geojson_geom = mapping(geom)

print(geojson_geom)

```

