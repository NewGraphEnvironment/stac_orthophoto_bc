---
title: "STAC create item "
format: html
jupyter: titiler  # Use your Conda environment
---

```{r env-activate}
library(reticulate)
reticulate::use_condaenv("stac", required = TRUE)
# reticulate::py_config()

```


```{python date-extract}
import re

def date_extract_after_utm(s):
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    return match.group(1) if match else None

# this is more flexible 
def date_extract_from_path(s):
    # Step 1: Try to extract YYYYMMDD or YYYY after _utmXX_
    match = re.search(r'_utm\d{1,2}_([0-9]{4,8})', s)
    if match:
        val = match.group(1)
        if val.isdigit():
            year = int(val[:4])
            if 2000 <= year <= 2050:
                return val

    # Step 2: Fallback: look for /YYYY/ in the path
    fallback = re.search(r'/([2][0-9]{3})/', s)
    if fallback:
        year = int(fallback.group(1))
        if 2000 <= year <= 2050:
            return str(year)

    return None


from datetime import datetime, timezone

def datetime_parse_item(s):
    if s is None:
        return None
    if len(s) == 8:
        return datetime.strptime(s, "%Y%m%d").replace(tzinfo=timezone.utc)
    elif len(s) == 4:
        return datetime.strptime(s, "%Y").replace(tzinfo=timezone.utc)
    return None


```


```{python create-stac-items}
import concurrent.futures
import pathlib
import json
from datetime import datetime
import pystac
import rasterio
import rio_stac.stac
import subprocess
import pandas as pd
import os
import argparse

# Parse CLI arguments
test_only = True
check_all = False if test_only else True

# Define paths
path_local = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"
path_collection = f"{path_local}/collection.json"
path_s3_stac = "https://stac-orthophoto-bc.s3.amazonaws.com"
path_s3_json = f"{path_s3_stac}/collection.json"
path_s3 = "https://nrs.objectstore.gov.bc.ca/gdwuts"
path_results_csv = "data/stac_geotiff_checks.csv"

# Load base collection
collection = pystac.Collection.from_file(path_collection)
collection.set_self_href(path_s3_json)

# Load list of image paths
with open("data/urls_list.txt") as f:
    path_items = f.read().splitlines()



# Load previous validation results
if os.path.exists(path_results_csv) and not check_all:
    df_existing = pd.read_csv(path_results_csv)
    done_urls = set(df_existing["url"])
else:
    df_existing = pd.DataFrame(columns=["url", "is_geotiff", "is_cog"])
    done_urls = set()

# Run validation only on new URLs unless check_all is set
urls_to_check = path_items if check_all else [u for u in path_items if u not in done_urls]

# Optional test mode overrides check_all and limits to 3
if test_only:
    check_all = False
    urls_to_check = urls_to_check[:3]

# Validate GeoTIFF and COG in one step
def check_geotiff_cog(url: str) -> dict:
    try:
        result = subprocess.run(
            ["rio", "cogeo", "validate", f"/vsicurl/{url}"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False
        )
        output = result.stdout.decode() + result.stderr.decode()
        return {
            "url": url,
            "is_geotiff": "is NOT a valid cloud optimized GeoTIFF" in output or "is a valid cloud optimized GeoTIFF" in output,
            "is_cog": "is a valid cloud optimized GeoTIFF" in output
        }
    except FileNotFoundError:
        raise RuntimeError("`rio cogeo` is not installed or not in PATH")


print(f"Checking {len(urls_to_check)} URLs{' (forced)' if check_all else ' (new only)'}...")

# Run checks
import tqdm
with concurrent.futures.ThreadPoolExecutor() as executor:
    new_results = list(tqdm.tqdm(executor.map(check_geotiff_cog, urls_to_check), total=len(urls_to_check), desc="Validating GeoTIFFs"))

# Combine and save
df_new = pd.DataFrame(new_results)
df_all = df_new if check_all else pd.concat([df_existing, df_new], ignore_index=True)
df_all.to_csv(path_results_csv, index=False)

# Create lookup for quick access
results_lookup = {
    row["url"].replace("https:/", "https://"): {"is_geotiff": row["is_geotiff"], "is_cog": row["is_cog"]}
    for _, row in df_all.iterrows()
}

# Define the processing function
def process_item(path_item: str) -> dict | None:
    href_item = path_item.replace("https:/", "https://")
    check = results_lookup.get(href_item)
    if check is None or not check["is_geotiff"]:
        print(f"Skipping unreadable GeoTIFF: {href_item}")
        return None

    item_id = path_item[len(path_s3):].replace("/", "-").removesuffix(".tif")

    item_time = datetime_parse_item(date_extract_from_path(path_item))

    media_type = (
        "image/tiff; application=geotiff; profile=cloud-optimized"
        if check["is_cog"] else
        "image/tiff; application=geotiff"
    )

    try:
        item = rio_stac.stac.create_stac_item(
            path_item,
            id=item_id,
            asset_media_type=media_type,
            asset_name='image',
            asset_href=href_item,
            with_proj=True,
            collection=collection.id,
            collection_url=path_s3_json,
            asset_roles=["data"]
        )
        item.datetime = item_time
        item.set_self_href(f"{path_s3_stac}/{item_id}.json")
        item.assets['image'].href = href_item

        # Save locally
        path_item_json = f"{path_local}/{item_id}.json"
        item.save_object(dest_href=path_item_json, include_self_link=False)

        return {
            "id": item_id,
            "json_path": path_item_json
        }
    except Exception as e:
        print(f"Error processing {href_item}: {e}")
        return None

# Run in parallel using threads to avoid rasterio multiprocessing issues
try:
    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = list(filter(None, tqdm.tqdm(executor.map(process_item, urls_to_check), total=len(urls_to_check), desc="Creating STAC Items")))
except Exception as e:
    print(f"Parallel execution failed: {e}")
    results = []

# Add all valid items to the collection
if results:
    for result in results:
        item = pystac.Item.from_file(result["json_path"])
        item.set_self_href(f"{path_s3_stac}/{result['id']}.json")  # <- critical
        item.clear_links("self")
        collection.add_item(item)


collection.save_object(dest_href=path_collection)


```


```{r json-clean, eval= FALSE}
# DELETE ATTACK - USE WITH CARE
# list all the jsons in the directory so we can delete them
path_base = "/Users/airvine/Projects/gis/stac_orthophoto_bc/prod/stac_orthophoto_bc"

j <- fs::dir_ls(path_base, glob = "*.json", recurse = TRUE)

fs::file_delete(j)

```

```{python validate}


for item in collection.get_all_items():
    try:
        item.validate()
        # print(f"✅ {item.id} is valid.")
    except Exception as e:
        print(f"❌ {item.id} failed validation: {e}")


# or 1 at a time
# item = Item.from_file(f"{path_local}/093-093h-2019-dem-bc_093h092_xli1m_utm10_2019.json")
# item.validate()
```


```{python, xyz, eval - FALSE}
import mercantile

# Bounds from Titiler response
west, south, east, north = -124.703866, 54.107650, -124.695957, 54.111509

# Choose a zoom level
zoom = 15

# Get a tile covering the center of the bounds
tile = mercantile.tile((west + east) / 2, (south + north) / 2, zoom)

print(f"Tile coordinates: Z={tile.z}, X={tile.x}, Y={tile.y}")
```

```{python query-s3-json}
import pystac

# Load the Collection JSON directly
collection = pystac.Collection.from_file("https://dev-imagery-uav-bc.s3.amazonaws.com/imagery_uav_bc.json")

# Define BC bounding box (approximate)
bbox_bc = [-139, 48, -114, 60]  # (West, South, East, North)

# Get all items linked in the collection
items = collection.get_all_items()

# Filter items that fall within BC bounds
bc_items = []
for item in items:
    # Check if item's bounding box intersects with BC
    item_bbox = item.bbox  # [west, south, east, north]
    if (
        item_bbox[2] >= bbox_bc[0] and  # Item east >= BC west
        item_bbox[0] <= bbox_bc[2] and  # Item west <= BC east
        item_bbox[3] >= bbox_bc[1] and  # Item north >= BC south
        item_bbox[1] <= bbox_bc[3]      # Item south <= BC north
    ):
        bc_items.append(item)

# Print matching items
for item in bc_items:
    print(f"Item: {item.id}")
    for asset_key, asset in item.assets.items():
        print(f"  - {asset_key}: {asset.href}")


```


```{python query-titiler}
import requests
import pystac

# Define BC bounding box (West, South, East, North)
bbox_bc = [-139, 48, -114, 60]

# Titiler endpoint
titiler_endpoint = "http://titiler-env.eba-s4jhubvr.us-west-2.elasticbeanstalk.com"

# Load the STAC Collection
collection_url = "https://dev-imagery-uav-bc.s3.amazonaws.com/imagery_uav_bc.json"
collection = pystac.Collection.from_file(collection_url)

# Get all items
items = collection.get_all_items()

# Filter items using /stac/bounds
matching_items = []
for item in items:
    stac_url = item.get_self_href()
    bounds_url = f"{titiler_endpoint}/stac/bounds?url={stac_url}"

    response = requests.get(bounds_url)
    if response.status_code == 200:
        item_bbox = response.json()["bounds"]  # [west, south, east, north]

        # Check intersection with BC bbox
        if (
            item_bbox[2] >= bbox_bc[0] and  # Item east >= BC west
            item_bbox[0] <= bbox_bc[2] and  # Item west <= BC east
            item_bbox[3] >= bbox_bc[1] and  # Item north >= BC south
            item_bbox[1] <= bbox_bc[3]      # Item south <= BC north
        ):
            matching_items.append((item.id, stac_url))

# Print matching items
print("Matching Items in BC:")
for item_id, stac_url in matching_items:
    print(f"- {item_id}: {stac_url}")


```

```{python footprint}

# here we explore how to update the footprints of our items so that we see where the raster has non-na values
# Remote URL wrapped in /vsicurl/ for GDAL
url = "/vsicurl/https://nrs.objectstore.gov.bc.ca/gdwuts/093/093h/2019/dem/bc_093h092_xli1m_utm10_2019.tif"
url = "/vsicurl/https://nrs.objectstore.gov.bc.ca/gdwuts/093/093h/2019/dem/bc_093h084_xli1m_utm10_2019.tif"

from stactools.core.utils.raster_footprint import RasterFootprint
from shapely.geometry import shape, mapping, Polygon

# Build footprint
rf = RasterFootprint.from_href(
    href=url,
    precision=5,                # Coarser geometry (faster, smaller)
    densification_factor=None, # Disable extra vertex interpolation
    densification_distance=None,
    simplify_tolerance=0.001,  # Simplify geometry slightly (in degrees) 0.001 ≈ 111 meters
    no_data=None,              # Use nodata from metadata (excludes it automatically)
    bands=[1]                  # Only use first band
)
geojson_dict = rf.footprint()

# Optional: turn into shapely geometry
geom = shape(geojson_dict)

# Convert back to GeoJSON dict
geojson_geom = mapping(geom)

print(geojson_geom)

```

